{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455dcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage, SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.tools import tool\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# Set your Serper tool for web search\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "\n",
    "# --- Database Configuration ---\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"supportvectors_db\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PORT = \"5432\"\n",
    "TABLE_NAME = 'shipping_rates'\n",
    "\n",
    "# Initialize the Ollama model and bind the tool\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm = ChatOllama(model=\"qwen3\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8dc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_md_file(filename: str) -> str:\n",
    "    \"\"\"Read a markdown file from the 'config' folder and return its content as a string.\"\"\"\n",
    "    config_path = os.path.join(\"config\", filename)\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def format_message(message_object) -> str:\n",
    "    \"\"\"\n",
    "    Converts a LangChain message object into a human-readable string.\n",
    "\n",
    "    Args:\n",
    "        message_object: An instance of AIMessage, HumanMessage, or ToolMessage.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string with a prefix (System:, User:, Tool:).\n",
    "    \"\"\"\n",
    "    if isinstance(message_object, AIMessage):\n",
    "        return f\"System: {message_object.content}\"\n",
    "    elif isinstance(message_object, HumanMessage) or isinstance(message_object, ToolMessage):\n",
    "        return f\"User: {message_object.content}\"\n",
    "    else:\n",
    "        # Handle any other message types gracefully\n",
    "        return f\"Unknown: {str(message_object.content)}\"\n",
    "    \n",
    "# Define the state of our graph\n",
    "class State(TypedDict):\n",
    "    system_prompt: str\n",
    "    formatted_messages: list[str]\n",
    "    messages: Annotated[list, operator.add]\n",
    "\n",
    "# --- Tool Section ---\n",
    "\n",
    "# This tool runs when called by the model.\n",
    "@tool()\n",
    "def human_tool(query: str):\n",
    "    \"\"\"Ask the human user a question when you need information that cannot be found via search.\n",
    "    \n",
    "    Use this to ask for:\n",
    "    - Package weight (if not searchable)\n",
    "    - Destination ZIP code (if they only provided a city name and search didn't help)\n",
    "    - Any clarification questions\n",
    "    \n",
    "    Args:\n",
    "        query: The specific question to ask the user\n",
    "        \n",
    "    Returns:\n",
    "        The user's response as a string\n",
    "    \"\"\"\n",
    "    print(f\"\\n🤖 AI is requesting Human assistance for: '{query}'\")\n",
    "    human_input = input(\"🧑‍💻 Your response: \")\n",
    "    print (f\"🧑‍💻 Human provided: '{human_input}'\\n\")\n",
    "    return human_input\n",
    "\n",
    "@tool\n",
    "def google_search(query: str, num: int = 3) -> str:\n",
    "    \"\"\"Search Google for information. Use this to look up ZIP codes or item weights.\n",
    "    \n",
    "    Examples of good queries:\n",
    "    - \"ZIP code for Albany New York\"\n",
    "    - \"MacBook Pro 15 weight in pounds\"\n",
    "    - \"average laptop weight\"\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string\n",
    "        num: Number of results to return (default 3)\n",
    "        \n",
    "    Returns:\n",
    "        Search results as a formatted string\n",
    "    \"\"\"\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "    params = {\"q\": query, \"num\" : num}\n",
    "    \n",
    "    print(f\"Performing Google search for: '{query}'\")\n",
    "    \n",
    "    response = requests.post(url, json=params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        return results.get(\"organic\", [])  # Extract search results\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\"\n",
    "    \n",
    "@tool()\n",
    "def shipping_rate_lookup(weight: float, destination_zip: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate shipping rate for a package.\n",
    "    \n",
    "    Only call this tool after you have confirmed BOTH parameters with the user.\n",
    "    \n",
    "    Args:\n",
    "        weight: Package weight in pounds (must be a positive number)\n",
    "        destination_zip: 5-digit US ZIP code (as a string)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with rate information including price, currency, weight, and destination\n",
    "    \"\"\"\n",
    "    print(f\"------Inside shipping_rate_lookup. weight: {weight} lbs to zip: {destination_zip}---\")\n",
    "    \n",
    "    DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "    try:\n",
    "        # The database stores weight as an integer string in the primary key.\n",
    "        lookup_weight = str(round(float(weight)))\n",
    "    except (ValueError, TypeError):\n",
    "        return {\"error\": \"Invalid weight provided\"}\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD,\n",
    "            port=DB_PORT\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Assumes the table has a 'service_lbs' column for the weight.\n",
    "        query = f\"SELECT * FROM {TABLE_NAME} WHERE service_lbs = %s\"\n",
    "        cur.execute(query, (lookup_weight,))\n",
    "        result = cur.fetchone()\n",
    "\n",
    "        if result:\n",
    "            colnames = [desc[0] for desc in cur.description]\n",
    "            rate_data = dict(zip(colnames, result))\n",
    "            rate_data['destination_zip'] = destination_zip # Add destination_zip to the result\n",
    "            print(f\"Found rate data: {rate_data}\")\n",
    "            return rate_data\n",
    "        else:\n",
    "            print(f\"No shipping rate data found: {rate_data}\")\n",
    "            return {\"error\": f\"No shipping rate found for weight {lookup_weight} lbs.\"}\n",
    "\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(f\"Database error: {error}\")\n",
    "        return {\"error\": f\"Database error: {error}\"}\n",
    "    finally:\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519fbdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Initialize MCP client\\nclient = MultiServerMCPClient(\\n    {\\n        \"shipping_rate_lookup\": {\\n            \"url\": \"http://127.0.0.1:9000/mcp/\",\\n            \"transport\": \"streamable_http\",\\n        }\\n    }\\n)\\n\\nmcp_tools = await client.get_tools()\\nprint(f\"---Available MCP tools: {mcp_tools}---\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Initialize MCP client\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"shipping_rate_lookup\": {\n",
    "            \"url\": \"http://127.0.0.1:9000/mcp/\",\n",
    "            \"transport\": \"streamable_http\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "mcp_tools = await client.get_tools()\n",
    "print(f\"---Available MCP tools: {mcp_tools}---\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac7da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [human_tool, google_search, shipping_rate_lookup] \n",
    "model = llm.bind_tools(tools)\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5a42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: State):\n",
    "    \"\"\"Router to decide the next step.\"\"\"\n",
    "    print(\"-------Inside AI router. ------\")\n",
    "    messages = state.get('messages', [])\n",
    "    if not messages:\n",
    "        # No messages, perhaps end or handle as an error\n",
    "        return END\n",
    "\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the last message has tool calls, route to the tool node\n",
    "    if getattr(last_message, 'tool_calls', None):\n",
    "        return \"call_tools\"\n",
    "    \n",
    "    # Otherwise, you can decide to continue the conversation or end.\n",
    "    # Here, we'll just end if there are no tool calls.\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bec255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GRAPH NODES ---\n",
    "def chatbot(state: State):\n",
    "    \"\"\"The chatbot node that calls the LLM.\"\"\"\n",
    "    print(100*\"=\")\n",
    "    \n",
    "    # This print statement is now safe and handles any message type with a .content attribute.\n",
    "    # print(f\"---🤖 Chatbot thinking... Reacting to message of type: {type(last_message).__name__}---\")\n",
    "    print(f\"---🤖 Chatbot thinking... Reacting to message of type: {state[\"formatted_messages\"]}--\")\n",
    "\n",
    "    # The graph state `messages` already contains the full history.\n",
    "    # We just need to add the system prompt for this specific invocation.\n",
    "    # messages_for_llm = [SystemMessage(content=state[\"system_prompt\"])] + state[\"messages\"]\n",
    "    messages_for_llm = [SystemMessage(content=state[\"system_prompt\"])] + state[\"messages\"]\n",
    "    print(100*\"-\")\n",
    "    \n",
    "    response = model.invoke(messages_for_llm)  \n",
    "    \n",
    "    print(f\"LLM Response: {response}\")\n",
    "    print(100*\"=\")\n",
    "        \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2c022a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GRAPH DEFINITION ---\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"chatbot\", chatbot)\n",
    "graph.add_node(\"call_tools\", tool_node)\n",
    "\n",
    "# Set the entry point and edges\n",
    "graph.set_entry_point(\"chatbot\")\n",
    "graph.add_conditional_edges(\"chatbot\", \n",
    "                            router, \n",
    "                            {\"call_tools\": \"call_tools\", \n",
    "                             END: END} \n",
    "                            )\n",
    "graph.add_edge(\"call_tools\", \"chatbot\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "400f7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready. Type 'quit' to exit.\n",
      "====================================================================================================\n",
      "---🤖 Chatbot thinking... Reacting to message of type: ['User: I want to ship a Mackbook Pro M1 to 12084']--\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LLM Response: content='<think>\\nOkay, let\\'s see. The user wants to ship a MacBook Pro M1 to ZIP code 12084. First, I need to check if I have all the necessary information. The destination ZIP is provided as 12084, which is a 5-digit code, so that\\'s good. Now, the weight of the MacBook Pro M1. The user didn\\'t mention the weight, so I need to find that.\\n\\nI should use the google_search tool to look up the weight of a MacBook Pro M1. The query could be \"MacBook Pro M1 weight in pounds\" to get the weight in pounds. Let me call that search. Once I get the weight, I can then use the shipping_rate_lookup tool with both weight and ZIP code. But wait, the user might have provided the ZIP code correctly, but maybe I should confirm. However, the ZIP is already 5 digits, so it\\'s valid. The main missing piece is the weight. So first, search for the weight, then proceed.\\n</think>\\n\\n' additional_kwargs={} response_metadata={'model': 'qwen3', 'created_at': '2025-10-11T07:38:52.03569Z', 'done': True, 'done_reason': 'stop', 'total_duration': 20703321958, 'load_duration': 2943029541, 'prompt_eval_count': 986, 'prompt_eval_duration': 7150643125, 'eval_count': 244, 'eval_duration': 10602211584, 'model_name': 'qwen3'} id='run--5e1d26d3-d2d0-4e7e-b3cc-6a0e8cca53a8-0' tool_calls=[{'name': 'google_search', 'args': {'query': 'MacBook Pro M1 weight in pounds'}, 'id': '47a946ce-c1b3-4760-a363-d8e1beb5e2db', 'type': 'tool_call'}] usage_metadata={'input_tokens': 986, 'output_tokens': 244, 'total_tokens': 1230}\n",
      "====================================================================================================\n",
      "-------Inside AI router. ------\n",
      "Performing Google search for: 'MacBook Pro M1 weight in pounds'\n",
      "====================================================================================================\n",
      "---🤖 Chatbot thinking... Reacting to message of type: ['User: I want to ship a Mackbook Pro M1 to 12084']--\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LLM Response: content='<think>\\nOkay, let me process this. The user wants to ship a MacBook Pro M1 to ZIP code 12084. The weight wasn\\'t provided, so I did a Google search for the MacBook Pro M1\\'s weight. The first result says it\\'s 3.0 pounds. The second result mentions 2.7lbs for the MBA and 3.5 lbs for the MBP, but the user specified M1, which might refer to the 13-inch model. The third result lists the 14-inch as 3.5 pounds. Since the user didn\\'t specify the model (13 or 14-inch), but mentioned \"Mackbook Pro M1,\" I should use the 3.0 pounds from the first result. However, the second result\\'s comment about 3.5 lbs for the MBP might be relevant. Wait, the first result\\'s title says \"MacBook Pro (13-inch, M1, 2020)\" with 3.0 pounds. The third result is for the 14-inch. Since the user didn\\'t specify the size, but the model is M1, maybe the 13-inch is the correct one. I\\'ll go with 3.0 pounds. Now, I need to confirm the weight with the user. Wait, the tools require me to use the shipping_rate_lookup once I have both parameters. But the user hasn\\'t confirmed the weight yet. Wait, the initial step was to gather weight and ZIP. The ZIP is 12084, and the weight is 3.0 lbs from the search. But according to the workflow, after getting both, I should confirm with the user. So I need to ask the user to confirm the weight and ZIP. Wait, the user provided the ZIP, but the weight was looked up. So now, I should confirm both. But the user might not have provided the weight, so I need to check if the search result is sufficient. The first result gives 3.0 pounds. But maybe the user wants to confirm. Wait, the guidelines say to call shipping_rate_lookup after confirming both parameters. So I need to present the weight and ZIP to the user for confirmation. Let me check the tools again. The shipping_rate_lookup requires weight and destination_zip. Since the weight was found via search, and the ZIP is provided, I should confirm with the user. So the next step is to ask the user to confirm the weight (3.0 lbs) and ZIP (12084). Then proceed to call the shipping_rate_lookup.\\n</think>\\n\\n' additional_kwargs={} response_metadata={'model': 'qwen3', 'created_at': '2025-10-11T07:39:23.463722Z', 'done': True, 'done_reason': 'stop', 'total_duration': 30483335208, 'load_duration': 91800958, 'prompt_eval_count': 1689, 'prompt_eval_duration': 2285494583, 'eval_count': 580, 'eval_duration': 28082629583, 'model_name': 'qwen3'} id='run--5bf3f118-0fa4-4153-8f9b-35dfd7efc457-0' tool_calls=[{'name': 'human_tool', 'args': {'query': 'Please confirm the weight is 3.0 pounds and the destination ZIP code is 12084.'}, 'id': '8d3e1588-bf2f-4fd1-a681-51786fcc8ce0', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1689, 'output_tokens': 580, 'total_tokens': 2269}\n",
      "====================================================================================================\n",
      "-------Inside AI router. ------\n",
      "\n",
      "🤖 AI is requesting Human assistance for: 'Please confirm the weight is 3.0 pounds and the destination ZIP code is 12084.'\n",
      "🧑‍💻 Human provided: 'yes. That is correct'\n",
      "\n",
      "====================================================================================================\n",
      "---🤖 Chatbot thinking... Reacting to message of type: ['User: I want to ship a Mackbook Pro M1 to 12084']--\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LLM Response: content=\"<think>\\nOkay, the user confirmed the weight as 3.0 pounds and the ZIP code 12084. Now I need to call the shipping_rate_lookup tool with these parameters. Let me make sure the parameters are correct. Weight is a number, and the ZIP is a 5-digit string. The function requires weight as a number and destination_zip as a string. So I'll format the tool call accordingly.\\n</think>\\n\\n\" additional_kwargs={} response_metadata={'model': 'qwen3', 'created_at': '2025-10-11T07:39:45.567644Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7203959625, 'load_duration': 74866250, 'prompt_eval_count': 2246, 'prompt_eval_duration': 930994875, 'eval_count': 123, 'eval_duration': 6160461542, 'model_name': 'qwen3'} id='run--a6d01a97-22f4-4e20-9abe-afe5ee0f342d-0' tool_calls=[{'name': 'shipping_rate_lookup', 'args': {'destination_zip': '12084', 'weight': 3}, 'id': 'a58bd969-6630-4acc-981c-1d83a93882e5', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2246, 'output_tokens': 123, 'total_tokens': 2369}\n",
      "====================================================================================================\n",
      "-------Inside AI router. ------\n",
      "------Inside shipping_rate_lookup. weight: 3.0 lbs to zip: 12084---\n",
      "Found rate data: {'service_lbs': '3', 'next_day_by_830_am__fedex_first_overnight': Decimal('77.77'), 'next_day_by_1030_am__fedex_priority_overnight': Decimal('46.77'), 'next_day_by_3_pm__fedex_standard_overnight': Decimal('44.68'), '_2day_by_1030_am__fedex_2day_am': Decimal('29.71'), '_2day_by_5_pm__fedex_2day': Decimal('26.68'), '_3day_by_5_pm__fedex_express_saver': Decimal('24.83'), 'destination_zip': '12084'}\n",
      "====================================================================================================\n",
      "---🤖 Chatbot thinking... Reacting to message of type: ['User: I want to ship a Mackbook Pro M1 to 12084']--\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LLM Response: content=\"<think>\\nOkay, the user wanted to ship a MacBook Pro M1 to ZIP code 12084. I first checked the weight using Google search and found it to be 3.0 pounds. The user confirmed that. Then I called the shipping_rate_lookup tool with weight=3 and destination_zip='12084'. The response from the tool provided various shipping options with their prices. Now, I need to present these options clearly to the user. Let me list each service with its price, making sure to mention the service names and the costs. Also, I should highlight the cheapest options if possible, but since the user didn't ask for that, just present all the rates as per the tool's response. Make sure the formatting is easy to read, maybe bullet points. Check for any errors in the numbers and ensure the ZIP code is correct. Alright, time to format the response.\\n</think>\\n\\nHere are the shipping rate options for your 3.0 lbs package to ZIP code 12084:\\n\\n- **FedEx First Overnight** (Next Day by 8:30 AM): $77.77  \\n- **FedEx Priority Overnight** (Next Day by 10:30 AM): $46.77  \\n- **FedEx Standard Overnight** (Next Day by 3:00 PM): $44.68  \\n- **FedEx 2Day AM** (2-Day by 10:30 AM): $29.71  \\n- **FedEx 2Day** (2-Day by 5:00 PM): $26.68  \\n- **FedEx Express Saver** (3-Day by 5:00 PM): $24.83  \\n\\nLet me know if you'd like help selecting the best option!\" additional_kwargs={} response_metadata={'model': 'qwen3', 'created_at': '2025-10-11T07:40:07.022708Z', 'done': True, 'done_reason': 'stop', 'total_duration': 21082017666, 'load_duration': 77118125, 'prompt_eval_count': 2512, 'prompt_eval_duration': 935883041, 'eval_count': 381, 'eval_duration': 20011858750, 'model_name': 'qwen3'} id='run--ab1c1505-1a1d-4a5f-85ba-6c882e18a41d-0' usage_metadata={'input_tokens': 2512, 'output_tokens': 381, 'total_tokens': 2893}\n",
      "====================================================================================================\n",
      "-------Inside AI router. ------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- UI DRIVER ---\n",
    "# This loop is still necessary to start each new conversation turn.\n",
    "print(\"Chatbot is ready. Type 'quit' to exit.\")\n",
    "\n",
    "#I want to ship a computer to new york\n",
    "#How much is to ship a Macbook Pro m1 to New York?\n",
    "#I want to ship a Mackbook Pro M1 to Albany, NY\n",
    "#I want to ship a Mackbook Pro M1 to 12084\n",
    "#How much is to ship to New York?\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"thread_123\"}}\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"👤 You: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # This kicks off the graph execution for the current turn\n",
    "    for chunk in app.stream(\n",
    "        {\"messages\": [user_input],\n",
    "         \"formatted_messages\": [f\"User: {user_input}\"],\n",
    "         \"system_prompt\": read_md_file(\"shipping_rate_lookup.md\")},\n",
    "        config=config,\n",
    "    ):\n",
    "        # Print the final output from the chatbot after the whole loop is done\n",
    "        if END in chunk:\n",
    "            final_message = chunk[END]['messages'][-1]\n",
    "            if final_message.content:\n",
    "                print(f\"🤖 AI: {final_message.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
